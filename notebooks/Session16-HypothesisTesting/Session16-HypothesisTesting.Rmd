---
title: 'Session 16: Hypothesis testing'
output:
  html_document:
    df_print: paged
---

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

Load the libraries.

```{r}
library(NHANES)
library(ggplot2)
library(dplyr)
library(tidyr)

```


# Coin flipping example

```{r}
# set the random seed so that the sample is identical each time
set.seed(1234567899)

nSamples <- 50000
sampleData_df <- #create a data frame with 1 variable representing the number of heads in 100 coin flips
  tibble(
    n_heads = rbinom(n = nSamples, size = 100, prob = 0.5)
  )
```

```{r}
#plot the data
sampleData_df %>% 
  ggplot(aes(n_heads)) +
  geom_histogram(bins = 100, binwidth = 0.5, center = 0) +
  geom_vline(aes(xintercept = 70), color = "blue") +
  labs(x = "Number of heads")
```

```{r}
#identify number of values where n_heads >= 70
print(paste("Number of random samples >= 70 heads):", sum(sampleData >= 70)))

#identify proportion of values where n_heads >= 70
print(paste("Proportion of random samples >= 70 heads):", mean(sampleData >= 70)))

#identify p-value for when n_heads >= 70
print(paste("Binomial distribution: p(X>=70|p=0.5):", 1 - pbinom(69, 100, 0.5)))
```

# NHANES example

Let's test a question about the NHANES dataset, using a sample of 250 individuals: Is regular physical activity related to body mass index?

```{r}
# set the random seed so that the sample is identical each time
set.seed(1234567899)

#create an adults only NHANES data frame where BMI and PhysActive are not missing
NHANES_adult <-
  NHANES %>% 
  filter(Age > 18 & !is.na(BMI) & !is.na(PhysActive)) 

# take a sample from the NHANES dataset
sampSize <- 250
NHANES_sample <- sample_n(NHANES_adult, size = sampSize)
```

```{r}
#plot the data
NHANES_sample %>% 
  ggplot(aes(x = PhysActive, y = BMI)) +
  geom_boxplot() + 
  labs(
    x = "Physically active?",
    y = "Body Mass Index (BMI)"
  )
  
```

```{r}
#summarize the data 
sampleSummary <- 
  NHANES_sample %>% 
  group_by(PhysActive) %>% 
  summarize(
    n = n(),
    mean = mean(BMI), 
    sd = sd(BMI)
  )

print(sampleSummary)

#calculate the mean difference between the two PhysActive groups

##method 1: use index numbers
meanDiff <- sampleSummary[1, 3] - sampleSummary[2, 3]
meanDiff

##method 2: reformat data and subtract columns (does not require identify index numbers but code is longer)
meanDiff <-
  sampleSummary %>% 
  select(PhysActive, mean) %>% 
  spread(key = PhysActive, value = mean) %>% 
  mutate(No - Yes) %>% 
  pull()
meanDiff
```

Let's perform permutation on the full NHANES_adult data set to assess the distribution of proportions under the null hypothesis that there is no relation between PhysActive and BMI. The NHANES_adult dataset is fairly large dataset so this analysis could take a couple of minutes to complete.  

```{r}
nSamples <- 2500

bmiData <-
  NHANES_adult %>% 
  select(BMI, PhysActive)

bmiDataShuffled <- bmiData #initially, assign unshuffled data
meanDiffSim <- array(NA, nSamples) #create an empty array with the size of nSamples

#create for loop 
for (i in 1:nSamples) {
  # shuffle the labels: take random samples from the data
  bmiDataShuffled$PhysActive <- sample(bmiDataShuffled$PhysActive)
  # compute the difference (t-statistic)
  simResult <- 
    t.test(
      bmiDataShuffled$BMI ~ bmiDataShuffled$PhysActive,
      var.equal = TRUE
    )
  #add the resulting t-statistic to the empty array created above 
  meanDiffSim[i] <- simResult$statistic
}

meanDiffSimDf <- tibble(meanDiffSim = meanDiffSim)
```

Let's compare the the mean difference in BMI between the groups we calculated on the unshuffuled data (i.e., the observed mean to difference) to the maximum mean difference we observed using permutation (i.e., under the null hypothesis of no difference).
```{r}
print(meanDiff)
print(paste('maximum observed different in permutation sample:',max(meanDiffSimDf$meanDiffSim)))
```

Letâ€™s compare the histogram of differences under the null hypothesis to the observed difference between the groups.

```{r}
meanDiffSimDf %>%
  ggplot(aes(meanDiffSim)) +
  geom_histogram(bins = 200) +
  geom_vline(aes(xintercept = meanDiff), color = "blue") +
  geom_histogram(
    data = tibble(meanDiffSim) %>% filter(meanDiffSim >= meanDiff), 
    aes(meanDiffSim), 
    bins = 200, 
    fill = "orange"
  ) +
  labs(x = "T stat: BMI difference between groups")

```

Above we used to permutation to create an empirical null distribution. Instead of using an empirical null distribution, it's common to use a theoretical null distribution.  For differences between means, the appropriate distribution is the Student's t distribution, with a number of degrees of freedom determined by the sample size; for two groups, since we are estimating the mean for each group, df = N - 2

Let's plot the t distribution over the empirical null distribution.

```{r}
#create a vector of density values for the t-distribution for mean differences ranging from -4 to 4
differences <- seq(-4, 4, 0.1)
tdist <- dt(differences, df = sampSize)

##plot the t-distribution over the simulated (empirical) null distribution
meanDiffSimDf %>% 
  ggplot(aes(meanDiffSim)) +
  geom_histogram(aes(y = ..density..), bins = 100) +
  geom_vline(aes(xintercept = meanDiff), color = "blue") +
  geom_line(
    data = tibble(tdist = tdist, differences = differences), 
    aes(differences, tdist), 
    color = "red", 
    size = 2
  ) +
  xlim(-4, 4) +
  labs(
    x = "T stat: BMI difference between groups"
  )
```

Let's plot the t-distribution with correct number of degrees of freedom (250 - 2), identifying the observed mean difference.

```{r}
##plot the t-distribution using the correct degrees of freedom 
dtfun <- function(differences) {
  return(dt(differences, df = 248))
}

meanDiffSimDf %>% 
  ggplot(aes(meanDiffSim)) +
  geom_line(
    data = tibble(tdist = tdist, differences = differences), 
    aes(differences, tdist), 
    color = "red", 
    size = 2
  ) +
  stat_function(
    fun = dtfun,
    xlim = c(meanDiff, 4),
    geom = "area", 
    fill = "orange"
  ) +
  geom_vline(aes(xintercept = meanDiff), color = "blue") +
  labs(x = "T stat: BMI difference between groups")
```

We can also plot the cumulative distributions against one another.

```{r}
meanDiffSimDf %>%
  ggplot(aes(meanDiffSim)) +
  stat_ecdf(aes(color = "black")) +
  geom_step(
    data = tibble(tdist = cumsum(tdist) / sum(tdist), differences = differences),
    aes(x, tdist, color = "green"), linetype = "dashed"
  ) +
  geom_vline(aes(xintercept = meanDiff, color = "blue")) +
  scale_color_manual(
    values = c("black", "blue", "green"), 
    labels = c("T distribution", "Observed difference", "Permutation")) +
  xlim(-4, 4) +
  xlab("T stat: BMI difference between groups") 
```


We can compute the likelihood of the observed difference under the null hypothesis. First, we'll use the theoretical t distribution.   To do this, we need to determine the probability of finding a value as large or larger than the observed value under this distribution.  To do this, we can use the pt() function which determines the cumulative probability of a particular value of the t distribution given a certain number of degrees of freedom.  In this case, we use df=248 since there are 250 observations and we are estimating two parameters (the means of each group).

We will just keep it easy and compute the t statistic using the built in t.test() function.

```{r}
ttestResult <- t.test(
  BMI ~ PhysActive, 
  data = NHANES_sample,
  var.equal = TRUE,
  alternative = 'greater' #one-tailed test
)

print(ttestResult)

lowerpt <- pt(ttestResult$statistic, sampSize - 2)
print(paste("lower tail probability (t distribution):", lowerpt[1]))
```

Since the total probability sums to 1, the probabilty of a value larger than our observed value is simply one minus the lower tail probability:

```{r}
upperpt <- 1 - lowerpt
print(paste("upper tail probability (t distribution):", upperpt[1]))
```

Let's compare that to the probability of finding a value as large as or larger than the observed value in the distribution of permutation results.

```{r}
print(
  paste(
    "Number of", nSamples, "permutations equal to or greater than t =",
    ttestResult$statistic, ":",
    sum(meanDiffSim >= ttestResult$statistic)
  )
)

upperpPerm <- mean(meanDiffSim >= ttestResult$statistic)

print(paste("upper tail probability (permutation distribution):", upperpPerm[1]))
```

What happens if we rerun the test as a two-tailed (non-directional) test?

```{r}
ttestResult <- 
  t.test(
    BMI ~ PhysActive,
    data = NHANES_sample,
    var.equal = TRUE,
    alternative = 'two.sided'
  )

print(ttestResult)
```

#### Neyman-Pearson

Let's simulate a situation in order to see how the different aspects of the Neyman-Pearson approach work together.

Let's simulate performance from two groups of subjects (say, Californians and Texans) on a test of marijuana sensitivity, which has a mean of 10 and a standard deviation of 1. We will start by simulating a dataset where the is no true difference.  

Because we will want to reuse this simulation, let's first create a function that performs the simulation.

```{r}
runSimulation <- function(
  #arguments for the function
  trueDifference, 
  nPerGroup, 
  alpha = 0.05, 
  nSims = 5000,
  print.output = FALSE) {
  
  pVals <- array(NA, nSims) #create an empty array for storing 5000 p-values 
  
  for (i in 1:nSims) {
    simData_CA <- rnorm(nPerGroup, mean = 10, sd = 1)
    simData_TX <- rnorm(nPerGroup, mean = 10 + trueDifference, sd = 1)
    t.result <- t.test(simData_CA, simData_TX, var.equal = TRUE)
    pVals[i] <- t.result$p.value
  }
  
  if (print.output) {
    print(paste(
      "proportion of significant results (over", nSims, "simulations):", 
      mean(pVals <= alpha)
    ))
    
  }
  
  return(mean(pVals <= alpha))
  
}

nPerGroup <- 20
trueDifference <- 0
sim.out <- runSimulation(trueDifference, nPerGroup, print.output = TRUE)
```

You can see that over many simulations, the probability of finding a significant result (which is a false positive, since there is no true difference) is very close to alpha.

Now let's see what happens when there truly is a difference between the groups.  First let's assume that it's a really large difference. 

```{r}
nPerGroup <- 20
trueDifference <- 1
sim.out <- runSimulation(trueDifference, nPerGroup, print.output = TRUE)
```

When the effect is large (in this case it's 1 standard deviation, which is a pretty large effect) then we will reject the null hypothesis most of the time - but not all of the time.

Let's see how the likelihood of finding a significant result changes as a function of the size of the effect. 

```{r}
trueDifferences <- seq(0, 2, 0.1)
proportionSignificant <- array(NA, length(trueDifferences))

for (i in 1:length(trueDifferences)) {
  proportionSignificant[i] <- runSimulation(trueDifferences[i], nPerGroup)
}
```

```{r}
tibble(
  trueDiff = trueDifferences,
  propSig = proportionSignificant
) %>% 
  ggplot(aes(trueDiff,propSig)) +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype = 'dashed') +
  labs(
    x = 'Size of true effect (in SD units)',
    y = 'Proportion of significant tests'
  )
```

What do you think happens if we increase the sample size? Let's try it with several different sample sizes to see what effect that has. This could take a couple of minutes to complete.

```{r}
trueDifferences <- seq(0, 2, 0.2)
sampleSizes <- c(20, 40, 60, 80)

#create empty arrays for storing the values we will generate
proportionSignificant <- array(NA, length(sampleSizes) * length(trueDifferences))
sampleSize <- array(NA, length(sampleSizes) * length(trueDifferences))
trueDifference <- array(NA, length(sampleSizes) * length(trueDifferences))
i <- 1

for (td in trueDifferences) {
  for (ss in sampleSizes) {
    sampleSize[i] <- ss
    trueDifference[i] <- td
    proportionSignificant[i] <- runSimulation(td, ss)
    i <- i + 1
  }
}
```

```{r}
#create a data frame of the results
df <- 
  tibble(
  trueDiff = trueDifference,
  propSig = proportionSignificant,
  sampleSize = as.factor(sampleSize)
)

#plot the results

ggplot(df,aes(trueDiff,propSig,color=sampleSize)) +
  geom_line(size=1.25) +
  xlab('Size of true effect (in SD units)') + ylab('Proportion of significant tests') +
  geom_hline(yintercept = 0.05,linetype='dashed')

```

What about changing alpha?  What effect does that have on power?  Let's vary alpha while holding sample size constant at 40.

```{r}
trueDifferences=seq(0,2,0.2)
sampleSize=40
alphas=c(0.001,0.005,0.01,0.05,0.1)
proportionSignificant=array(NA,length(alphas)*length(trueDifferences))
alpha=array(NA,length(alphas)*length(trueDifferences))
trueDifference=array(NA,length(alphas)*length(trueDifferences))
i=1
for (td in trueDifferences){
  for (a in alphas) {
    alpha[i]=a
    trueDifference[i]=td
    proportionSignificant[i]=runSimulation(td,ss,alpha=a)
    i=i+1
  }
}

```

```{r}
df=data.frame(trueDiff=trueDifference,propSig=proportionSignificant,alpha=as.factor(alpha))
ggplot(df,aes(trueDiff,propSig,color=alpha)) +
  geom_line(size=1.25) +
  xlab('Size of true effect (in SD units)') + ylab('Proportion of significant tests')
```

Let's see what happens with a really small effect as the sample size gets large.  This could take a few minutes.

```{r}
sampleSizes=2**seq(4,18)
proportionSignificant=array(NA,length(sampleSizes))
for (i in 1:length(sampleSizes)){
  proportionSignificant[i]=runSimulation(0.01,sampleSizes[i])
}
```

```{r}
df=data.frame(sampleSize=sampleSizes,propSig=proportionSignificant)
ggplot(df,
       aes(sampleSize,propSig)) +
  geom_line() + ylim(0,1) + scale_x_log10() +
  xlab('log Sample size') + ylab('Proportion of significant tests') +
  geom_hline(yintercept = 0.05,linetype='dashed')


print(df)
```

### Positive predictive value

The positive predictive value is the proportion of positive outcomes (i.e. rejection of the null) that are correct.  This moves with three things:
- alpha (false positive rate)
- power (1 - false negative rate)
- prior probability that the hypothesis is correct

Let's plot an example

```{r}

ppv = function(alpha,power,prior){
  return ((power*prior)/(power*prior + alpha))
}
powerVals=c(0.1,0.5,0.8,0.9)
priorVals=seq(0.1,1,0.1)
ppvVal=array(NA,length(powerVals)*length(priorVals))
i=1
powerVal=array(NA,length(powerVals)*length(priorVals))
priorVal=array(NA,length(powerVals)*length(priorVals))

for (pwr in powerVals){
  for (prior in priorVals){
    ppvVal[i]=ppv(0.05,pwr,prior)
    powerVal[i]=pwr
    priorVal[i]=prior
    i=i+1
  }
}

df=data.frame(ppv=ppvVal,fdr=1-ppvVal,prior=priorVal,power=as.factor(powerVal))

ggplot(df,aes(priorVal,ppv,color=power)) +
  geom_line(size=1.5) + ylab('Positive Predictive Value') +
  xlab('Prior probability of true effect') + ylim(0,1)
ggplot(df,aes(priorVal,fdr,color=power)) +
  geom_line(size=1.5) + ylab('False discovery rate') +
  xlab('Prior probability of true effect') + ylim(0,1)


```

### Schizophrenia example


```{r}
oddsRatioChr6=1.167
pSZ=7.2/1000
pGene=1 - 0.763

# we just assume here that the unexposed rates are the population prevalence

oddsNoGene=pSZ/(1-pSZ)
1/oddsNoGene
oddsGene = oddsRatioChr6*oddsNoGene
1/oddsGene

# this one is for rs171748
oddsRatioChr6=1.08 
pSZ=7.2/1000
pGene=1 -  0.471

# we just assume here that the unexposed rates are the population prevalence

oddsNoGene=pSZ/(1-pSZ)
1/oddsNoGene
oddsGene = oddsRatioChr6*oddsNoGene
1/oddsGene

```

### simulating multiple testing

Here we simulate the effects of running a large number of statistical tests.  Let's say we are going to run a million statistical tests, but that the results are actually normally distributed with a mean of zero.  That is, there is no effect to be found. How often would we actually find an effect?

```{r}
nTests=10000
uncAlpha=0.05
uncOutcome=replicate(nTests,sum(rnorm(nTests)<(qnorm(uncAlpha))))

print(paste('uncorrected:',mean(uncOutcome>0)))

corAlpha=0.05/nTests

corOutcome=replicate(nTests,sum(rnorm(nTests)<(qnorm(corAlpha))))
print(paste('corrected:',mean(corOutcome>0)))

```

### Randomization example

```{r}

roundToNearest5 <- function(x,base=5){ 
        return(base*round(x/base))
} 

set.seed(123456)
df=data.frame(group=as.factor(c(rep('FB',5),rep('XC',5))),
              squat=roundToNearest5(c(rnorm(5)*30 + 300,rnorm(5)*30 + 140)))


df = df %>% mutate(scrambledGroup=sample(group))
df = df %>% mutate(scrambledGroup2=sample(group))
df

tt=t.test(squat~group,data=df,var.equal=TRUE)
tt

tt_scram=t.test(squat~scrambledGroup,data=df,var.equal=TRUE)
tt_scram

tt_scram2=t.test(squat~scrambledGroup2,data=df,var.equal=TRUE)
tt_scram2

```

Now randomize order 10000 times to get histogram

```{r}

getScrambledTtest = function(dfScram){
  dfScram$group = sample(dfScram$group)
  tt=t.test(squat~group,data=dfScram,var.equal=TRUE)
  return(tt$statistic)
}

ttestScrambledResults = replicate(10000,getScrambledTtest(df))

ggplot(data.frame(ttest=ttestScrambledResults),aes(ttest)) + 
  geom_histogram(bins=100) +
  geom_vline(xintercept = tt$statistic,color='blue')

mean(ttestScrambledResults)
max(ttestScrambledResults)
mean(ttestScrambledResults>=tt$statistic)
```

